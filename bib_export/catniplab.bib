% Encoding: UTF-8

@InProceedings{Nassar2019a,
  author    = {Josue Nassar and Scott Linderman and Il Memming Park and Monica Bugallo},
  title     = {Tree-structured locally linear dynamics model to uproot Bayesian neural data analysis},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2019},
}

@InProceedings{Nassar2018b,
  author        = {Josue Nassar and Scott W. Linderman and Monica Bugallo and Il Memming Park},
  title         = {Tree-Structured Recurrent Switching Linear Dynamical Systems for Multi-Scale Modeling},
  booktitle     = {International Conference on Learning Representations (ICLR)},
  year          = {2019},
  month         = nov,
  abstract      = {Many real-world systems studied are governed by complex, nonlinear dynamics. By modeling these dynamics, we can gain insight into how these systems work, make predictions about how they will behave, and develop strategies for controlling them. While there are many methods for modeling nonlinear dynamical systems, existing techniques face a trade off between offering interpretable descriptions and making accurate predictions. Here, we develop a class of models that aims to achieve both simultaneously, smoothly interpolating between simple descriptions and more complex, yet also more accurate models. Our probabilistic model achieves this multi-scale property through a hierarchy of locally linear dynamics that jointly approximate global nonlinear dynamics. We call it the tree-structured recurrent switching linear dynamical system. To fit this model, we present a fully-Bayesian sampling procedure using Polya-Gamma data augmentation to allow for fast and conjugate Gibbs sampling. Through a variety of synthetic and real examples, we show how these models outperform existing methods in both interpretability and predictive capability.},
  archiveprefix = {arXiv},
  day           = {30},
  eprint        = {1811.12386},
  keywords      = {neural-dynamics, statistical-neuroscience},
  url           = {https://openreview.net/forum?id=HkzRQhR9YX},
  code          = {https://github.com/catniplab/tree_structured_rslds},
}

@InProceedings{Mofakham2019b,
  author    = {Sima Mofakham and Adam Fry and Joseph Adachi and Nathan Winans and Justine Liang and Bradley Ashcroft and Himanshu Sharma and Susan Fiore and Il Memming Park and Charles Mikell},
  title     = {Electrophysiological prognostication of functional cortical integrity after traumatic brain injury},
  booktitle = {American Association of Neurological Surgeons (AANS)},
  year      = {2019},
  note      = {(submitted)},
  owner     = {memming},
}

@InProceedings{Mofakham2019a,
  author    = {Sima Mofakham and Adam Fry and Joseph Adachi and Bradley Ashcroft and Nathan Winans and Justine Liang and Himanshu Sharma and Susan Fiore and Il Memming Park and Charles Mikell},
  title     = {Recovery of consciousness after traumatic brain injury: Biomarkers and a mechanistic model},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2019},
}

@Article{Hocker2019a,
  author    = {David Hocker and Il Memming Park},
  title     = {Myopic control of neural dynamics},
  journal   = {PLOS Computational Biology},
  year      = {2019},
  note      = {(in press)},
  abstract  = {Manipulating the dynamics of neural systems through targeted stimulation is a frontier of research and clinical neuroscience; however, the control schemes considered for neural systems are mismatched for the unique needs of manipulating neural dynamics. An appropriate control method should respect the variability in neural systems, incorporating moment to moment ``input'' to the neural dynamics and behaving based on the current neural state, irrespective of the past trajectory. We propose such a controller under a nonlinear state-space feedback framework that steers one dynamical system to function as through it were another dynamical system entirely. This ``myopic'' controller is formulated through a novel variant of a model reference control cost that manipulates dynamics in a step-wise manner, omitting the need to pre-calculate a rigid and computationally costly neural feedback control solution. To demonstrate the breadth of this control's utility, two examples with distinctly different applications in neuroscience are studied. First an unhealthy motor-like system containing an unwanted beta-oscillation spiral attractor is controlled to function as a healthy motor system, a relevant clinical example for neurological disorders. Second, we show the myopic control's utility to probe the causal dynamics of cognitive processes by transforming a winner-take-all decision-making system to operate as a robust neural integrator of evidence.},
  doi       = {10.1101/241299},
  owner     = {memming},
  timestamp = {2017.12.31},
  url       = {https://www.biorxiv.org/content/early/2017/12/30/241299},
}

@InProceedings{Bobkov2019a,
  author    = {Yuriy Bobkov and Il Park and Brenden T. Michaelis and Tom Matthews and Matthew A. Reidenbach and Jos\'e C. Pr\'incipe and Barry Ache},
  title     = {Coding spatiotemporal characteristics of odor signals},
  booktitle = {Association for Chemoreception (AChemS) Annual Meeting},
  year      = {2019},
  abstract  = {Odor signals are generally assumed to be processed based on input from canonical tonically active primary olfactory receptor neurons (ORNs). Recent evidence suggests, however, that a subpopulation of ORNs consists of intrinsically rhythmically active, ‘bursting’ ORNs (bORNS) with unique functional properties. (1) They encode olfactory information by having their rhythmicity entrained by the odor stimulus; they don’t discharge phaso-tonically as do canonical ORNs.  (2) As a population they can accurately and instantaneously encode the interval since the last odor encounter up to 10s of seconds using information inherent in the olfactory modality.  (3) Particularly interesting is that they can do this without the need to implicate a memory mechanism.  Given the structure inherent in turbulent odor plumes, the ability of bORNs to encode the time between ‘whiffs’ would provide a novel way of navigating turbulent plumes. Indeed, recent computational modeling shows an ‘animat’ can navigate odor plumes with considerable more efficiency using odor time than odor concentration.   To further support and expand this computational model, we are now using combined physiological and morphological analysis to characterize the molecular receptive range of bORNs, map their central projection, and begin to identify the strategy for synaptic processing of bORN-derived information at the first olfactory relay.  While more work clearly needs to be done, and hopefully our work will encourage others to do that, our findings suggest encoding odor time is a fundamental feature of olfaction that potentially can be used to navigate odor plumes in animals as diverse as crustaceans and mammals.},
  owner     = {memming},
}

@InProceedings{Zhao2018a,
  author    = {Yuan Zhao and Il Memming Park},
  title     = {Accessing neural states in real time: recursive variational Bayesian dual estimation},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2018},
}

@Unpublished{Sokol2018a,
  author        = {Piotr Sokol and Il Memming Park},
  title         = {Information geometry of orthogonal initializations and training},
  note          = {(under review)},
  month         = oct,
  year          = {2018},
  abstract      = {Recently mean field theory has been successfully used to analyze properties
of wide, random neural networks. It has given rise to a prescriptive theory for
initializing neural networks, which ensures that the \$\ell\_2\$ norm of the
backpropagated gradients is bounded, and training is orders of magnitude
faster. Despite the strong empirical performance of this class of
initializations, the mechanisms by which they confer an advantage in the
optimization of deep neural networks are poorly understood. Here we show a
novel connection between the maximum curvature of the optimization landscape
(gradient smoothness) as measured by the Fisher information matrix and the
maximum singular value of the input-output Jacobian. Our theory partially
explains why neural networks that are more isometric can train much faster.
Furthermore, we experimentally investigate the benefits of maintaining
orthogonality throughout training, from which we conclude that manifold
constrained optimization of weights performs better regardless of the
smoothness of the gradients. Finally we show that critical orthogonal
initializations do not trivially give rise to a mean field limit of
pre-activations for each layer.},
  archiveprefix = {arXiv},
  day           = {9},
  eprint        = {1810.03785},
  journal       = {ArXiv e-prints},
  keywords      = {deep-learning, machine-learning},
  posted-at     = {2018-10-10 15:14:09},
  url           = {http://arxiv.org/abs/1810.03785},
}

@InProceedings{Nassar2018a,
  author    = {Nassar, Josue and Linderman, Scott and Zhao, Yuan and Bugallo, M\'onica and Park, Il Memming},
  title     = {Learning structured neural dynamics from single trial population recording},
  booktitle = {52nd Asilomar Conference on Signals, Systems and Computers},
  year      = {2018},
  abstract  = {To understand the complex nonlinear dynamics of neural circuits, we fit a structured state-space model called \textit{tree-structured recurrent switching linear dynamical system} (TrSLDS) to noisy high-dimensional neural time series.
TrSLDS is a multi-scale hierarchical generative model for the state-space dynamics where each node of the latent tree captures locally linear dynamics.
TrSLDS can be learned efficiently and in a  fully Bayesian manner using Gibbs sampling.
We showcase TrSLDS' potential of inferring low-dimensional interpretable dynamical systems on a variety of examples.},
  file      = {Nassar2018a.pdf},
  owner     = {memming},
  timestamp = {2018.07.18},
}

@Article{Kirschen2018a,
  author   = {Kirschen, Gregory W. and Ge, Shaoyu and Park, Il Memming},
  title    = {Probability of viral labeling of neural stem cells {\it in vivo}},
  journal  = {Neuroscience Letters},
  year     = {2018},
  abstract = {In the neuroscience field over the past several decades, viral vectors have become powerful gene delivery systems to study neural populations of interest. For neural stem cell (NSC) biology, such viruses are often used to birth-date and track NSCs over developmental time in lineage tracing experiments. Yet, the probability of successful infection of a given stem cell in vivo remains unknown. This information would be helpful to inform investigators interested in titrating their viruses to selectively target sparsely-populated clusters of cells in the nervous system. Here, we describe a novel approach to calculate the probability of successful viral infection of NSCs using experimentally-derived cell cluster data from our newly-developed method to sparsely label adult NSCs, and a simple statistical derivation. Others interested in precisely defining their viral infection efficiency can use this method for a variety of basic and translational studies.},
  doi      = {10.1016/j.neulet.2018.05.016},
  keywords = {neural-stem-cells, probability-estimation},
}

@InProceedings{Hocker2018a,
  author    = {David Hocker and Il Memming Park},
  title     = {Myopic Control: A New Control Objective for Neural Population Dynamics},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2018},
}

@Article{Esfahany2018b,
  author        = {Esfahany, Kathleen and Siergiej, Isabel and Zhao, Yuan and Park, Il Memming},
  title         = {Organization of neural population code in mouse visual system},
  journal       = {eNeuro},
  year          = {2018},
  pages         = {0414--17},
  month         = jul,
  abstract      = {The mammalian visual system consists of several anatomically distinct areas, layers, and cell types. To understand the role of these subpopulations in visual information processing, we analyzed neural signals recorded from excitatory neurons from various anatomical and functional structures. For each of 186 mice, one of six genetically tagged cell-types and one of six visual areas were targeted while the mouse was passively viewing various visual stimuli. We trained linear classifiers to decode one of six visual stimulus categories with distinct spatiotemporal structures from the population neural activity. We found that neurons in both the primary visual cortex and secondary visual areas show varying degrees of stimulus-specific decodability, and neurons in superficial layers tend to be more informative about the stimulus categories. Additional decoding analyses of directional motion were consistent with these findings. We observed synergy in the population code of direction in several visual areas suggesting area-specific organization of information representation across neurons. These differences in decoding capacities shed light on the specialized organization of neural information processing across anatomically distinct subpopulations, and further establish the mouse as a model for understanding visual perception.
},
  archiveprefix = {bioRxiv},
  day           = {6},
  doi           = {10.1523/ENEURO.0414-17.2018},
  publisher     = {Society for Neuroscience},
  url           = {https://www.biorxiv.org/content/early/2018/03/25/220558}
}

@InProceedings{Esfahany2018a,
  author    = {Kathleen Esfahany and Isabel Siergiej and Yuan Zhao and Il Memming Park},
  title     = {Organization of neural population code in mouse visual system},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2018},
}

@InProceedings{Bobkov2018a,
  author    = {Yuriy Bobkov and Il Memming Park and Brenden T. Michaelis and Tom Matthews and Matthew A. Reidenbach and Jos\'e C. Pr\'incipe and Barry Ache},
  title     = {Rhythmically discharging olfactory receptor neurons can encodethe spatiotemporal characteristics of odor signals within complexfluid environments},
  booktitle = {European Chemoreception Research Organization (ECRO)},
  year      = {2018},
  address   = {Würzburg, Germany},
  month     = sep,
  abstract  = {Sensory signals, including olfactory signals, are typically encoded by tonic receptor neurons. A significant portion of the olfactory receptor neurons in some species is intrinsically rhythmically active or ‘bursting’ (bORNS). Rather than phaso-tonically discharging to the odor onset as characteristic of tonic olfactory receptor neurons, in bORNs the frequency of their inherent burst is entrained by the intermittency inherent in turbulent odor plumes. Each bORN responds to a relatively narrow range of stimulus frequencies (intermittency) based on their inherent rate of bursting discharge and the phase dependency of their response to odor stimulation. Using computational and analytical approaches we have shown that heterogeneous populations of such uncoupled oscillatory neurons have the capacity to reliably encode the temporal properties of intermittent odor signals as long as seconds to many tens of seconds that characterize natural odor plumes. In the present study, we expand our current understanding bORN-based encoding by characterizing the molecular receptive range of bORNs, mapping their central projection, and beginning to identify the strategy for the synaptic processing of bORN-derived information at the first olfactory relay.
},
  comment   = {Poster session: Theme II - Vertebrate taste and olfaction: periphery (Th-P-054)},
  owner     = {memming},
  timestamp = {2019.01.09},
  url       = {https://coms.events/ECRO2018/data/abstracts/en/abstract_0089.html},
}

@Unpublished{Zhao2017c,
  author               = {Zhao, Yuan and Park, Il Memming},
  title_old                = {Recursive Variational Bayesian Dual Estimation for Nonlinear Dynamics and {Non-Gaussian} Observations},
  title                = {Variational Recursive Dual Filtering},
  year                 = {2017},
  abstract             = {State space models provide an interpretable framework for complex time series
by combining an intuitive dynamical system model with a probabilistic
observation model. We developed a flexible online learning framework for latent
nonlinear state dynamics and filtered latent states. Our method utilizes the
stochastic gradient variational Bayes method to jointly optimize the parameters
of the nonlinear dynamics, observation model, and the recognition model. Unlike
previous approaches, our framework can incorporate non-trivial observation
noise models and infer in real-time. We test our method on point process
observations driven by continuous attractor dynamics, demonstrating its ability
to recover the phase portrait, filtered trajectory, and produce long-term
predictions for neuroscience applications.},
  archiveprefix        = {arXiv},
  citeulike-article-id = {14403579},
  citeulike-linkout-0  = {http://arxiv.org/abs/1707.09049},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1707.09049},
  comment              = {submitted to ICML 2018; NIPS 2017},
  day                  = {27},
  eprint               = {1707.09049},
  journal              = {ArXiv e-prints},
  keywords             = {bayesian-filter, dual-estimation, nonlinear-dynamics, poisson-observation, state-space, time-series},
  owner                = {memming},
  posted-at            = {2017-07-31 12:08:02},
  note		       = {(under review)}
}

@InProceedings{Zhao2017b,
  author    = {Yuan Zhao and Jacob Yates and Il Memming Park},
  title     = {Low-dimensional state-space trajectory of choice at the population level in area {MT}},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2017},
  owner     = {memming},
  timestamp = {2017.02.11},
}

@InProceedings{Zhao2017a,
  author    = {Yuan Zhao and Il Memming Park},
  title     = {Gotta infer’em all: dynamical features from neural trajectories},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2017},
  owner     = {memming},
  timestamp = {2017.02.11},
}

@Article{Zhao2016a,
  author               = {Zhao, Yuan and Park, Il Memming},
  title                = {Variational Latent {G}aussian Process for Recovering Single-Trial Dynamics from Population Spike Trains},
  journal              = {Neural Computation},
  year                 = {2017},
  volume               = {29},
  number               = {5},
  month                = may,
  abstract             = {When governed by underlying low-dimensional dynamics, the interdependence of simultaneously recorded populations of neurons can be explained by a small number of shared factors, or a low-dimensional trajectory. Recovering these latent trajectories, particularly from single-trial population recordings, may help us understand the dynamics that drive neural computation. However, due to the biophysical constraints and noise in the spike trains, inferring trajectories from data is a challenging statistical problem in general. Here, we propose a practical and efficient inference method, the variational latent gaussian process (vLGP). The vLGP combines a generative model with a history-dependent point process observation, together with a smoothness prior on the latent trajectories. The vLGP improves on earlier methods for recovering latent trajectories, which assume either observation models inappropriate for point processes or linear dynamics. We compare and validate vLGP on both simulated data sets and population recordings from the primary visual cortex. In the V1 data set, we find that vLGP achieves substantially higher performance than previous methods for predicting omitted spike trains, as well as capturing both the toroidal topology of visual stimuli space and the noise correlation. These results show that vLGP is a robust method with the potential to reveal hidden neural dynamics from large-scale neural recordings.},
  archiveprefix        = {arXiv},
  citeulike-article-id = {14025142},
  citeulike-linkout-0  = {http://arxiv.org/abs/1604.03053},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1604.03053},
  day                  = {22},
  doi                  = {10.1162/NECO_a_00953},
  eprint               = {1604.03053},
  keywords             = {bayesian, computational-neuroscience, gaussian-process, glm, latent-dynamics, latent-variable, spike-train, statistical-neuroscience, statistics, torus, variational-bayes},
  posted-at            = {2016-04-30 16:02:05},
  primaryclass         = {stat.ML},
  youtube              = {https://www.youtube.com/watch?v=CrY5AfNH1ik},
  code                 = {https://github.com/catniplab/vlgp},
}

@Article{Yates2017a,
  author               = {Yates, Jacob L. and Park, Il Memming and Katz, Leor N. and Pillow, Jonathan W. and Huk, Alexander C.},
  title                = {Functional dissection of signal and noise in {MT} and {LIP} during decision-making},
  journal              = {Nature Neuroscience},
  year                 = {2017},
  month                = jul,
  issn                 = {1097-6256},
  volume               = {20},
  pages                = {1285-–1292},
  citeulike-article-id = {14398857},
  citeulike-linkout-0  = {http://dx.doi.org/10.1038/nn.4611},
  day                  = {24},
  doi                  = {10.1038/nn.4611},
  keywords             = {functional-connectivity, glm, lip, mt, noise-correlation, psychophysics},
  posted-at            = {2017-07-24 16:43:28},
}

@Unpublished{Park2017a,
    author = {Park, Il Memming and Pillow, Jonathan W.},
    day = {22},
    doi = {10.1101/178418},
    archiveprefix = {bioRxiv},
    journal = {bioRxiv},
    keywords = {bayesian-efficient-coding, preprint, tuning-curve},
    month = aug,
    pages = {178418+},
    publisher = {Cold Spring Harbor Labs Journals},
    title = {Bayesian Efficient Coding},
    url = {http://www.biorxiv.org/content/early/2017/08/22/178418},
    year = {2017},
    note = {(under review)}
}

@InProceedings{Hocker2017b,
  author    = {David Hocker and Il Memming Park},
  title     = {Instability of the generalized linear model for spike trains},
  booktitle = {Computational and Systems Neuroscience ({COSYNE})},
  year      = {2017},
  owner     = {memming},
  timestamp = {2017.02.11},
}

@InProceedings{Hocker2017a,
  author    = {David Hocker and Il Memming Park},
  title     = {Multistep inference for generalized linear spiking models curbs runaway excitation},
  booktitle = {8th International IEEE EMBS Conference On Neural Engineering},
  year      = {2017},
  pages     = {613--616},
  month     = may,
  publisher = {IEEE},
  doi       = {10.1109/ner.2017.8008426},
  file      = {Hocker2017a.pdf},
  isbn      = {978-1-5090-4603-4},
  keywords  = {autoregressive, glm, multi-step-prediction, spike-train, time-series},
  location  = {Shanghai, China},
  owner     = {memming},
  timestamp = {2017.02.11},
}

@InProceedings{Zhao2016d,
  author        = {Zhao, Yuan and Park, Il Memming},
  title         = {Interpretable Nonlinear Dynamic Modeling of Neural Trajectories},
  booktitle     = {Advances in Neural Information Processing Systems (NIPS)},
  year          = {2016},
  abstract      = {A central challenge in neuroscience is understanding how neural system
implements computation through its dynamics. We propose a nonlinear
time series model aimed at characterizing interpretable dynamics
from neural trajectories. Our model assumes low-dimensional continuous
dynamics in a finite volume. It incorporates a prior assumption about
globally contractional dynamics to avoid overly enthusiastic extrapolation
outside of the support of observed trajectories. We show that our
model can recover qualitative features of the phase portrait such
as attractors, slow points, and bifurcations, while also producing
reliable long-term future predictions in a variety of dynamical models
and in real neural data.},
  archiveprefix = {arXiv},
  eprint        = {1608.06546},
  keywords      = {autoregressive, bifurcation, chaos, continuous-attractor, dynamics, neural-dynamics, nips, oscillation, tensorflow},
  primaryclass  = {q-bio.QM},
  youtube	= {https://www.youtube.com/watch?v=7oWRZRpaq_I},
  url = {https://papers.nips.cc/paper/6543-interpretable-nonlinear-dynamic-modeling-of-neural-trajectories},
}

@INPROCEEDINGS{Zhao2016c,
  author = {Yuan Zhao and Il Memming Park},
  title = {Variational inference of latent {G}aussian neural dynamics},
  booktitle = {International Conference on Machine Learning (ICML) Workshop on Computational
	Biology},
  year = {2016}
}

@INPROCEEDINGS{Zhao2016b,
  author = {Yuan Zhao and Il Memming Park},
  title = {Inferring low-dimensional network dynamics with variational latent
	{G}aussian process},
  booktitle = {Organization for Computational Neuroscience (CNS)},
  year = {2016}
}

@INPROCEEDINGS{Dikecligil2016,
  author = {Gulce Nazli Dikecligil and Dustin Graham and Il Memming Park and
	Alfredo Fontanini},
  title = {Layer Specific Sensorimotor Activity in the Gustatory Cortex of Licking
	Mice},
  booktitle = {Society for Neuroscience},
  year = {2016},
  owner = {memming}
}

@INPROCEEDINGS{Yates2015a,
  author = {Jacob Yates and Evan Archer and Alexander C. Huk and Il Memming Park},
  title = {Canonical correlations reveal co-variability between spike trains
	and local field potentials in area {MT}},
  booktitle = {Organization for Computational Neuroscience (CNS)},
  year = {2015}
}

@INPROCEEDINGS{Wu2015b,
  author = {Anqi Wu and Il Memming Park and Jonathan Pillow},
  title = {Convolutional spike-triggered covariance analysis for neural subunit
	models},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year = {2015},
  owner = {memming}
}

@InProceedings{Wu2015a,
  author    = {Anqi Wu and Il Memming Park and Jonathan Pillow},
  title     = {Convolutional spike-triggered covariance analysis for estimating subunit models},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2015},
  owner     = {memming},
}

@InProceedings{Park2015a,
  author    = {Il Memming Park and Jacob Yates and Alex Huk and Jonathan Pillow},
  title     = {Dynamic correlations between visual and decision areas during perceptual decision-making},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2015},
  owner     = {memming},
}

@UNPUBLISHED{Archer2015a,
  author = {Archer, Evan and Park, Il Memming and Buesing, Lars and Cunningham,
	John and Paninski, Liam},
  title = {Black box variational inference for state space models},
  journal = {ArXiv e-prints},
  year = {2015},
  month = nov,
  abstract = {Latent variable time-series models are among the most heavily used
	tools from machine learning and applied statistics. These models
	have the advantage of learning latent structure both from noisy observations
	and from the temporal ordering in the data, where it is assumed that
	meaningful correlation structure exists across time. A few highly-structured
	models, such as the linear dynamical system with {linear-Gaussian}
	observations, have closed-form inference procedures (e.g. the Kalman
	Filter), but this case is an exception to the general rule that exact
	posterior inference in more complex generative models is intractable.
	Consequently, much work in time-series modeling focuses on approximate
	inference procedures for one particular class of models. Here, we
	extend recent developments in stochastic variational inference to
	develop a `black-box' approximate inference technique for latent
	variable models with latent dynamical structure. We propose a structured
	Gaussian variational approximate posterior that carries the same
	intuition as the standard Kalman filter-smoother but, importantly,
	permits us to use the same inference approach to approximate the
	posterior of much more general, nonlinear latent variable generative
	models. We show that our approach recovers accurate estimates in
	the case of basic models with closed-form posteriors, and more interestingly
	performs well in comparison to variational approaches that were designed
	in a bespoke fashion for specific non-conjugate models.},
  archiveprefix = {arXiv},
  citeulike-article-id = {13850684},
  citeulike-linkout-0 = {http://arxiv.org/abs/1511.07367},
  citeulike-linkout-1 = {http://arxiv.org/pdf/1511.07367},
  day = {23},
  eprint = {1511.07367},
  keywords = {deep-learning, machine-learning, stochastic-gradient-descent-algorithm,
	time-series, variational-bayes},
  posted-at = {2016-02-23 15:24:40},
  primaryclass = {stat.ML},
  priority = {2},
}

@INPROCEEDINGS{Yates2014a,
  author = {Jacob L. Yates and Leor N. Katz and Il Memming Park and Jonathan
	W. Pillow and Alexander C. Huk},
  title = {Correlations and choice probabilities in simultaneously recorded
	{MT} and {LIP} neurons},
  booktitle = {Society for Neuroscience},
  year = {2014}
}

@Article{Park2014d,
  author               = {Park, Il Memming and Meister, Miriam L. R. and Huk, Alexander C. and Pillow, Jonathan W.},
  title                = {Encoding and decoding in parietal cortex during sensorimotor decision-making},
  journal              = {Nature Neuroscience},
  year                 = {2014},
  volume               = {17},
  number               = {10},
  pages                = {1395--1403},
  month                = oct,
  issn                 = {1097-6256},
  citeulike-article-id = {13342234},
  citeulike-linkout-0  = {http://dx.doi.org/10.1038/nn.3800},
  doi                  = {10.1038/nn.3800},
  file                 = {Park2014d_typofixed.pdf},
  keywords             = {computational-neuroscience, decision-making, glm, lip, monkey, neural-code, neural-decoding},
  posted-at            = {2014-08-31 22:37:11},
}

@INPROCEEDINGS{Park2014c,
  author = {Park, Il Memming and Seth, Sohan and Van Vaerenbergh, Steven},
  title = {Probabilistic Kernel Least Mean Squares Algorithms},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing
	(ICASSP)},
  year = {2014},
  doi = {10.1109/ICASSP.2014.6855214},
  url = {http://gtas.unican.es/pub/393},
  publisher = {IEEE}
}

@InProceedings{Park2014b,
  author    = {Il Memming Park and Evan Archer and Kenneth Latimer and Jonathan Pillow},
  title     = {Scalable nonparametric models for binary spike patterns},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2014},
}

@ARTICLE{Park2014a,
  author = {Il Memming Park and Yuriy V. Bobkov and Barry W. Ache and Jos\'e
	C. Pr\'incipe},
  title = {Intermittency coding in the primary olfactory system: A neural substrate
	for olfactory scene analysis},
  journal = {The Journal of Neuroscience},
  year = {2014},
  volume = {34},
  pages = {941--952},
  number = {3},
  month = jan,
  abstract = {The spatial and temporal characteristics of the visual and acoustic
	sensory input are indispensable attributes for animals to perform
	scene analysis. In contrast, research in olfaction has focused almost
	exclusively on how the nervous system analyzes the quality and quantity
	of the sensory signal and largely ignored the spatiotemporal dimension
	especially in longer time scales. Yet, detailed analyses of the turbulent,
	intermittent structure of water- and air-borne odor plumes strongly
	suggest that spatio-temporal information in longer time scales can
	provide major cues for olfactory scene analysis for animals. We show
	that a bursting subset of primary olfactory receptor neurons ({bORNs})
	in lobster has the unexpected capacity to encode the temporal properties
	of intermittent odor signals. Each {bORN} is tuned to a specific
	range of stimulus intervals, and collectively {bORNs} can instantaneously
	encode a wide spectrum of intermittencies. Our theory argues for
	the existence of a novel peripheral mechanism for encoding the temporal
	pattern of odor that potentially serves as a neural substrate for
	olfactory scene analysis.},
  day = {15},
  doi = {10.1523/jneurosci.2204-13.2014},
  issn = {1529-2401},
  keywords = {neural-code, odor-plume, olfactory, uncoupled-oscillator},
  publisher = {Society for Neuroscience}
}

@InProceedings{Huk2014a,
  author    = {Alexander Huk and Jacob Yates and Leor Katz and Il Memming Park and Jonathan Pillow},
  title     = {Dissociated functional significance of choice-related activity across the primate dorsal stream},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2014},
}

@Article{Archer2014a,
  author        = {Archer, Evan and Park, Il Memming and Pillow, Jonathan},
  title         = {{Bayes}ian Entropy Estimation for Countable Discrete Distributions},
  journal       = {Journal of Machine Learning Research},
  year          = {2014},
  volume        = {15},
  pages         = {2833--2868},
  archiveprefix = {arXiv},
  eprint        = {1302.0328},
  keywords      = {bayesian, entropy-estimation, nonparametric-bayes, pitman-yor-process},
  url           = {http://jmlr.org/papers/v15/archer14a.html},
  abstract      = {We consider the problem of estimating Shannon's entropy $H$ from discrete data, in cases where the number of possible symbols is unknown or even countably infinite. The {Pitman-Yor} process, a generalization of Dirichlet process, provides a tractable prior distribution over the space of countably infinite discrete distributions, and has found major applications in Bayesian non-parametric statistics and machine learning. Here we show that it also provides a natural family of priors for Bayesian entropy estimation, due to the fact that moments of the induced posterior distribution over $H$ can be computed analytically. We derive formulas for the posterior mean (Bayes' least squares estimate) and variance under Dirichlet and {Pitman-Yor} process priors. Moreover, we show that a fixed Dirichlet or {Pitman-Yor} process prior implies a narrow prior distribution over $H$, meaning the prior strongly determines the entropy estimate in the under-sampled regime. We derive a family of continuous mixing measures such that the resulting mixture of {Pitman-Yor} processes produces an approximately flat prior over $H$. We show that the resulting {Pitman-Yor} Mixture ({PYM}) entropy estimator is consistent for a large class of distributions. We explore the theoretical properties of the resulting estimator, and show that it performs well both in simulation and in application to real data.},
  archiveprefix        = {arXiv},
  citeulike-article-id = {12071222},
  citeulike-linkout-0  = {http://arxiv.org/abs/1302.0328},
  day                  = {2},
  eprint               = {1302.0328},
  primaryclass         = {cs.IT},
}

@INPROCEEDINGS{Yates2013b,
  author = {Jacob Yates and Il Memming Park and Lawrence Cormack and Jonathan
	Pillow and Alexander Huk},
  title = {Precise characterization of dorsal stream neural activity during
	decision making},
  booktitle = {Society for Neuroscience},
  year = {2013}
}

@InProceedings{Yates2013a,
  author    = {Jacob Yates and Il Memming Park and Lawrence Cormack and Jonathan Pillow and Alexander Huk},
  title     = {Precise characterization of multiple {LIP} neurons in relation to stimulus and behavior},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2013},
}

@InProceedings{Pillow2013a,
  author    = {Jonathan Pillow and Il Memming Park},
  title     = {Beyond {B}arlow: A {Bayes}ian theory of efficient neural coding},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2013},
}

@Unpublished{Park2013g,
  author               = {Park, Il Memming and Seth, Sohan and Van Vaerenbergh, Steven},
  title                = {{Bayes}ian Extensions of Kernel Least Mean Squares},
  month                = oct,
  year                 = {2013},
  abstract             = {The kernel least mean squares ({KLMS}) algorithm is a computationally
	efficient nonlinear adaptive filtering method that "kernelizes" the
	celebrated (linear) least mean squares algorithm. We demonstrate
	that the least mean squares algorithm is closely related to the Kalman
	filtering, and thus, the {KLMS} can be interpreted as an approximate
	Bayesian filtering method. This allows us to systematically develop
	extensions of the {KLMS} by modifying the underlying state-space
	and observation models. The resulting extensions introduce many desirable
	properties such as "forgetting", and the ability to learn from discrete
	data, while retaining the computational simplicity and time complexity
	of the original algorithm.},
  archiveprefix        = {arXiv},
  citeulike-article-id = {12732257},
  citeulike-linkout-0  = {http://arxiv.org/abs/1310.5347},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1310.5347},
  day                  = {20},
  eprint               = {1310.5347},
  journal              = {ArXiv e-prints},
  keywords             = {adaptive-filter, bayesian, kernel-method, klms, online-algorithm, poisson-observation},
  posted-at            = {2013-10-23 12:18:03},
  primaryclass         = {st.ML},
}

@INPROCEEDINGS{Park2013f,
  author = {Il Memming Park and Evan Archer and Nicholas Priebe and Jonathan
	W. Pillow},
  title = {Spectral methods for neural characterization using generalized quadratic
	models},
  url = {http://papers.nips.cc/paper/4993-spectral-methods-for-neural-characterization-using-generalized-quadratic-models},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year = {2013},
  owner = {memming}
}

@INPROCEEDINGS{Park2013e,
  author = {Il Memming Park and Evan Archer and Kenneth Latimer and Jonathan
	W. Pillow},
  title = {Universal models for binary spike patterns using centered {D}irichlet
	processes},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  url = {http://papers.nips.cc/paper/5050-universal-models-for-binary-spike-patterns-using-centered-dirichlet-processes},
  year = {2013},
  owner = {memming}
}

@INPROCEEDINGS{Park2013c,
  author = {Il Memming Park and Evan Archer and Jonathan Pillow},
  title = {{B}ayesian entropy estimators for spike trains},
  booktitle = {Computational Neuroscience (CNS)},
  year = {2013}
}

@InProceedings{Park2013b,
  author    = {Il Memming Park and Evan Archer and Nicholas Priebe and Jonathan Pillow},
  title     = {Got a moment or two? {N}eural models and linear dimensionality reduction},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2013},
}

@ARTICLE{Park2013a,
  author = {Park, Il Memming and Seth, Sohan and Paiva, Antonio R. C. and Li,
	Lin and Principe, Jose C.},
  title = {Kernel methods on spike train space for neuroscience: a tutorial},
  journal = {IEEE Signal Processing Magazine},
  year = {2013},
  volume = {30},
  pages = {149--160},
  number = {4},
  month = jul,
  abstract = {Over the last decade several positive definite kernels have been proposed
	to treat spike trains as objects in Hilbert space. However, for the
	most part, such attempts still remain a mere curiosity for both computational
	neuroscientists and signal processing experts. This tutorial illustrates
	why kernel methods can, and have already started to, change the way
	spike trains are analyzed and processed. The presentation incorporates
	simple mathematical analogies and convincing practical examples in
	an attempt to show the yet unexplored potential of positive definite
	functions to quantify point processes. It also provides a detailed
	overview of the current state of the art and future challenges with
	the hope of engaging the readers in active participation.},
  day = {24},
  doi = {10.1109/msp.2013.2251072},
  eprint = {1302.5964},
  issn = {1053-5888}
}

@INBOOK{Paiva2011,
  title = {Instantaneous cross-correlation analysis of neural ensembles with
	high temporal resolution},
  publisher = {Wiley},
  year = {2013},
  editor = {Dario Farina and Winnie Jensen and Metin Akay},
  author = {Ant\'onio R. C. Paiva and Il Park and Jos\'e C. Pr\'incipe and Justin
	Sanchez},
  booktitle = {Neural engineering applied to neurorehabilitation}
}

@INPROCEEDINGS{Archer2013d,
  author = {Evan Archer and Il Memming Park and Jonathan W. Pillow},
  title = {{Bayes}ian entropy estimation for binary spike train data using parametric
	prior knowledge},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  url = {http://papers.nips.cc/paper/4873-bayesian-entropy-estimation-for-binary-spike-train-data-using-parametric-prior-knowledge},
  year = {2013},
  owner = {memming}
}

@Article{Archer2013c,
  author               = {Archer, Evan and Park, Il Memming and Pillow, Jonathan},
  title                = {{Bayes}ian and Quasi-{Bayes}ian Estimators for Mutual Information from Discrete Data},
  journal              = {Entropy},
  year                 = {2013},
  volume               = {15},
  number               = {5},
  pages                = {1738--1755},
  month                = may,
  abstract             = {Mutual information ({MI}) quantifies the statistical dependency between
	a pair of random variables, and plays a central role in the analysis
	of engineering and biological systems. Estimation of {MI} is difficult
	due to its dependence on an entire joint distribution, which is difficult
	to estimate from samples. Here we discuss several regularized estimators
	for {MI} that employ priors based on the Dirichlet distribution.
	First, we discuss three {quasi-Bayesian} estimators that result
	from linear combinations of Bayesian estimates for conditional and
	marginal entropies. We show that these estimators are not in fact
	Bayesian, and do not arise from a well-defined posterior distribution
	and may in fact be negative. Second, we show that a fully Bayesian
	{MI} estimator proposed by Hutter (2002), which relies on a fixed
	Dirichlet prior, exhibits strong prior dependence and has large bias
	for small datasets. Third, we formulate a novel Bayesian estimator
	using a {mixture-of-Dirichlets} prior, with mixing weights designed
	to produce an approximately flat prior over {MI}. We examine the
	performance of these estimators with a variety of simulated datasets
	and show that, surprisingly, {quasi-Bayesian} estimators generally
	outperform our Bayesian estimator. We discuss outstanding challenges
	for {MI} estimation and suggest promising avenues for future research.},
  citeulike-article-id = {12335521},
  citeulike-linkout-0  = {http://dx.doi.org/10.3390/e15051738},
  citeulike-linkout-1  = {http://www.mdpi.com/1099-4300/15/5/1738},
  citeulike-linkout-2  = {http://www.mdpi.com/1099-4300/15/5/1738/pdf},
  day                  = {10},
  doi                  = {10.3390/e15051738},
  keywords             = {bayesian, entropy-estimation, estimation, information-theory, mutual-information},
  posted-at            = {2013-05-10 23:03:57},
}

@InProceedings{Archer2013b,
  author    = {Evan Archer and Il Memming Park and Jonathan Pillow},
  title     = {Semi-parametric {Bayes}ian entropy estimation for binary spike trains},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2013},
}

@INPROCEEDINGS{Park2012e,
  author = {Il Memming Park and Miriam L. R. Meister and Alexander C. Huk and
	Jonathan W. Pillow},
  title = {Deciphering the code for sensorimotor decision-making at the level
	of single neurons in parietal cortex},
  booktitle = {Society for Neuroscience},
  year = {2012},
  note = {(oral presentation)},
  owner = {memming}
}

@Unpublished{Park2012c,
  author        = {Il Memming Park and Marcel Nassar and Mijung Park},
  title         = {Active {Bayes}ian Optimization: Minimizing Minimizer Entropy},
  month         = feb,
  year          = {2012},
  abstract      = {The ultimate goal of optimization is to find the minimizer of a target
	{function.However}, typical criteria for active optimization often
	ignore the uncertainty about the minimizer. We propose a novel criterion
	for global optimization and an associated sequential active learning
	strategy using Gaussian {processes.Our} criterion is the reduction
	of uncertainty in the posterior distribution of the function minimizer.
	It can also flexibly incorporate multiple global minimizers. We implement
	a tractable approximation of the criterion and demonstrate that it
	obtains the global minimizer accurately compared to conventional
	Bayesian optimization criteria.},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2012arXiv1202.2143M},
  archiveprefix = {arXiv},
  eprint        = {1202.2143},
  journal       = {ArXiv e-prints},
  keywords      = {Statistics - Methodology, Computer Science - Learning, Statistics - Machine Learning},
  primaryclass  = {stat.ME},
}

@InProceedings{Park2012b,
  author    = {Il Memming Park and Jonathan Pillow},
  title     = {{Bayes}ian spike-triggered covariance and the elliptical {LNP} model},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2012},
  owner     = {memming},
}

@Article{Park2012a,
  author    = {Il Memming Park and Sohan Seth and Murali Rao and Jos\'e C. Pr\'incipe},
  title     = {Strictly positive definite spike train kernels for point process divergences},
  journal   = {Neural Computation},
  year      = {2012},
  volume    = {24},
  number    = {8},
  pages     = {2223--2250},
  month     = aug,
  doi       = {10.1162/NECO_a_00309},
  issue     = {8},
  owner     = {memming},
  timestamp = {2010.02.20},
  archiveprefix        = {arXiv},
  eprint               = {1302.5964},
  primaryclass         = {q-bio.NC},
}

@ARTICLE{Li2012a,
  author = {Lin Li and Il Memming Park and Austin Brockmeier and Badong Chen
	and Sohan Seth and Joseph T. Francis and Justin C. Sanchez and Jos\'e
	C. Pr\'incipe},
  title = {Adaptive Inverse Control of Neural Spatiotemporal Spike Patterns
	with a Reproducing Kernel {H}ilbert Space ({RKHS}) Framework},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  year = {2012},
  volume = {21},
  pages = {532--543},
  number = {4},
  doi = {10.1109/TNSRE.2012.2200300},
  issn = {1534-4320},
  keywords = {Hilbert spaces;MIMO systems;adaptive control;adaptive signal processing;bioelectric
	phenomena;decoding;medical control systems;medical signal processing;multidimensional
	signal processing;neurophysiology;nonlinear control systems;signal
	representation;spatiotemporal phenomena;MIMO adaptive inverse control
	scheme;Schoenberg kernel maps;adaptive inverse control;control procedure;decoding
	accuracy;electrical stimulation;elicited system response;generalized
	linear model;linear algorithm;multidimensional time-varying signal
	representation;multiple-input multiple-output adaptive inverse control
	scheme;neural perturbations;neural signal representation;neural spatiotemporal
	spike patterns;neural stimulation;neural system plasticity;neurons;nonlinear
	neural system control;population spiking activity;realistic synthetic
	neural circuit;reproducing kernel Hilbert space framework;spike train
	RKHS;spikernel model;spiking responses;standard control methodology;system
	control problem;target system response;time-invariant homogeneous
	model;Adaptation models;Decoding;Integrated circuit modeling;Kernel;MIMO;Timing;Vectors;Adaptive
	inverse control;Schoenberg kernel;neural stimulation;spike timing
	representations}
}

@ARTICLE{Li2011a,
  author = {Lin Li and Il Park and Sohan Seth and Justin C. Sanchez and Jos\'e
	C. Pr\'incipe},
  title = {Functional Connectivity Dynamics Among Cortical Neurons: A Dependence
	Analysis},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  year = {2012},
  volume = {20},
  pages = {18--30},
  number = {1},
  month = jan,
  doi = {10.1109/TNSRE.2011.2176749},
  issn = {1534-4320},
  keywords = {behavior task;cortical neurons;cross correlation;dependence analysis;firing
	rates;food reaching task;functional connectivity dynamics;mean square
	contingency;monkey cortex;movement states;mutual information;neural
	assembly functional connectivity;neural ensemble recordings;pairwise
	functional connectivity;phase synchronization;phase-based metrics;robust
	estimators;statistical analysis;temporal resolutions;time 100 ms
	to 1000 ms;bioelectric potentials;biomechanics;brain;correlation
	methods;estimation theory;medical signal processing;neurophysiology;statistical
	analysis;synchronisation;},
  owner = {memmingpark},
  timestamp = {2010.12.30}
}

@ARTICLE{Bobkov2012a,
  author = {Yuriy Bobkov and Il Park and Kirill Ukhanov and Jos\'e C. Pr\'incipe
	and Barry W. Ache},
  title = {Cellular basis for response diversity in the olfactory periphery},
  journal = {PLoS One},
  year = {2012},
  volume = {7},
  pages = {e34843+},
  number = {4},
  month = apr,
  abstract = {An emerging idea in olfaction is that temporal coding of odor specificity
	can be intrinsic to the primary olfactory receptor neurons ({ORNs}).
	As a first step towards understanding whether lobster {ORNs} are
	capable of generating odor-specific temporal activity and what mechanisms
	underlie any such heterogeneity in discharge pattern, we characterized
	different patterns of activity in lobster {ORNs} individually and
	ensemble using patch-clamp recording and calcium imaging. We demonstrate
	that lobster {ORNs} show tonic excitation, tonic inhibition, phaso-tonic
	excitation, and bursting, and that these patterns are faithfully
	reflected in the calcium signal. We then demonstrate that the various
	dynamic patterns of response are inherent in the cells, and that
	this inherent heterogeneity is largely determined by heterogeneity
	in the underlying intrinsic conductances.},
  day = {13},
  doi = {10.1371/journal.pone.0034843}
}

@INPROCEEDINGS{Archer2012b,
  author = {Evan Archer and Il Memming Park and Jonathan W. Pillow},
  title = {{Bayes}ian estimation of discrete entropy with mixtures of stick
	breaking priors},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year = {2012},
  url = {https://papers.nips.cc/paper/4521-bayesian-estimation-of-discrete-entropy-with-mixtures-of-stick-breaking-priors},
  owner = {memming}
}

@InProceedings{Archer2012a,
  author    = {Evan Archer and Il Memming Park and Jonathan Pillow},
  title     = {{Bayes}ian entropy estimation for infinite neural alphabets},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  url = {http://papers.nips.cc/paper/4521-bayesian-estimation-of-discrete-entropy-with-mixtures-of-stick-breaking-priors},
  year      = {2012},
  owner     = {memming},
}

@ARTICLE{Seth2011a,
  author = {Sohan Seth and Murali Rao and Il Park and Jos\'e C. Pr\'incipe},
  title = {A Unified Framework for Quadratic Measures of Independence},
  journal = {IEEE Transactions on Signal Processing},
  year = {2011},
  volume = {59},
  pages = {3624--3635},
  month = aug,
  issue = {8},
  doi = {10.1109/TSP.2011.2153197},
  owner = {memming},
  timestamp = {2010.07.27}
}

@InProceedings{Park2011d,
  author               = {Park, Il and Seth, Sohan and Rao, Murali and Principe, Jos\'e C.},
  title                = {Estimation of symmetric chi-square divergence for point processes},
  booktitle            = {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  year                 = {2011},
  pages                = {2016--2019},
  month                = may,
  publisher            = {IEEE},
  citeulike-article-id = {11334584},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/ICASSP.2011.5946907},
  citeulike-linkout-1  = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5946907},
  doi                  = {10.1109/ICASSP.2011.5946907},
  isbn                 = {978-1-4577-0538-0},
  issn                 = {1520-6149},
  keywords             = {divergence, hypothesis-test, point-process},
  posted-at            = {2012-09-27 14:09:53},
}

@INPROCEEDINGS{Park2011c,
  author = {Il Memming Park and Jonathan W. Pillow},
  title = {{Bayes}ian Spike Triggered Covariance Analysis},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  url = {http://papers.nips.cc/paper/4411-bayesian-spike-triggered-covariance-analysis},
  year = {2011},
  owner = {memming},
  timestamp = {2011.10.24}
}

@CONFERENCE{Park2011b,
  author = {Il Memming Park and Sohan Seth and Jos\'e C. Pr\'incipe},
  title = {Spike Train Kernel Methods for Neuroscience},
  booktitle = {Joint Statistical Meeting},
  year = {2011},
  abstract = {Positive definite kernels has been widely used in the context of machine
	learning by the, so called, kernel machines such as the support vector
	machine and the kernel principal component analysis. An attractive
	property of a kernel machine is that it can be applied to arbitrary
	spaces as long as appropriate kernel is provided. We have developed
	spike train kernels and analyzed their properties in the context
	of two-sample problem, probability embedding as well as regression
	and classification. We discuss strictly positive definite kernels
	that provide theoretical foundation for its power.},
  owner = {memming},
  timestamp = {2011.07.14}
}

@InProceedings{Park2011a,
  author    = {Il Memming Park and Miriam Meister and Alexander Huk and Jonathan W Pillow},
  title     = {Detailed encoding and decoding of choice-related information from {LIP} spike trains},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational and Systems Neuroscience ({COSYNE})},
  year      = {2011},
  owner     = {memming},
}

@CONFERENCE{Li2011b,
  author = {Lin Li and Il Memming Park and Sohan Seth and John Choi and Joseph
	T. Francis and Justin C. Sanchez and Jos\'e C. Pr\'incipe},
  title = {An adaptive decoder from spike trains to micro-stimulation using
	kernel least-mean-square ({KLMS}) algorithm},
  booktitle = {IEEE International Workshop on Machine Learning for Signal Processing (MLSP)},
  doi = {10.1109/MLSP.2011.6064603},
  year = {2011}
}

@INPROCEEDINGS{Seth2010a,
  author = {Sohan Seth and Il Park and Austin J. Brockmeier and Mulugeta Semework
	and John Choi and Joe Francis and Jos\'e C. Pr\'incipe},
  title = {A novel family of non-parametric cumulative based divergences for
	point processes},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  url = {http://papers.nips.cc/paper/4126-a-novel-family-of-non-parametric-cumulative-based-divergences-for-point-processes},
  year = {2010},
  pages = {2119--2127},
  owner = {memming},
  timestamp = {2010.02.18}
}

@BOOK{Principe2010,
  title = {Information Theoretic Learning},
  publisher = {Springer},
  year = {2010},
  author = {Jos\'e C. Pr\'incipe},
  isbn = {1441915699},
  url = {http://www.springer.com/us/book/9781441915696},
  owner = {memming},
  timestamp = {2009.05.19}
}

@INPROCEEDINGS{Park2010b,
  author = {Il Park and Jos\'e C. Pr\'incipe},
  title = {Quantification of Inter-trial Non-stationarity in Spike Trains from
	Periodically Stimulated Neural Cultures},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing
	(ICASSP)},
  year = {2010},
  pages = {5442--5445},
  note = {Special session on Multivariate Analysis of Brain Signals: Methods
	and Applications},
  doi = {10.1109/ICASSP.2010.5494920},
  owner = {memming},
  timestamp = {2009.12.15}
}

@PhdThesis{Park2010a,
  author    = {Il Memming Park},
  title     = {Capturing spike train similarity structure: A point process divergence approach},
  school    = {The University of Florida},
  year      = {2010},
  month     = {Aug},
  abstract  = {Neurons mostly communicate via stereotypical events called action
	potentials (or spikes for short) giving rise to a time series called
	the neural spike train. Spike trains are random in nature, and hence
	one needs to deal with the probability law over the spike trains
	space (point process). Rich statistical descriptors are a prerequisite
	for statistical learning in the spike train domain; this provides
	necessary analysis tools for neural decoding, change detection, and
	neuron model fitting. The first and second order statistics prevalently
	used in neuroscience -- such as mean firing rate function, and correlation
	function -- do not fully describe the randomness, thus are partial
	statistics. However, restricting the study to these basic statistics
	implicitly limit what can be discovered. We propose three families
	of statistical divergences that enable non-Poisson, and more over,
	distribution-free spike train analysis. We extend the Kolmogorov-Smirnov
	test, $\phi$-divergence, and kernel based divergence to point processes.
	This is possible through the development of novel mathematical foundations
	for point process representations.
	
	Compared to the similarity or distance measures for spike trains that
	assumes predefined stochasticity and hence are not flexible, divergences
	applied to sets of spike trains capture the underlying probability
	law and measures the statistical similarity. Therefore, divergences
	are more robust, and assumption free. We apply the methodology on
	real data from neuronal cultures as well as anesthetized animals
	for neuroscience and neuroengineering applications posed as statistical
	inferences to evaluate their usefulness.},
  owner     = {memming},
  timestamp = {2016.10.24},
}

@CONFERENCE{Paiva2010c,
  author = {Ant\'onio R. C. Paiva and Il Park},
  title = {Which measure should we use for unsupervised spike train learning?},
  booktitle = {Statistical Analysis of Neuronal Data (SAND5)},
  year = {2010},
  owner = {memming},
  timestamp = {2010.05.24}
}

@INBOOK{Paiva2010b,
  title = {Inner Products for Representation and Learning in the Spike Train
	Domain},
  publisher = {Academic Press},
  year = {2010},
  editor = {Karim G. Oweiss},
  author = {Ant\'onio R. C. Paiva and Il Park and Jos\'e C. Pr\'incipe},
  booktitle = {Statistical Signal Processing for Neuroscience},
  isbn = {978-0123750273},
  owner = {memming},
  timestamp = {2010.01.15}
}

ARTICLE{Paiva2008d,
  author = {Ant\'onio R. C. Paiva and Il Park and Jos\'e C. Pr\'incipe},
  title = {Optimization in Reproducing Kernel {H}ilbert Spaces of Spike Trains},
  journal = {(book chapter in press)},
  owner = {memming},
  timestamp = {2008.10.08}
}

@ARTICLE{Paiva2009a,
  author = {Ant\'onio R. C. Paiva and Il Park and Jos\'e C. Pr\'incipe},
  title = {A Comparison of Binless Spike Train Measures},
  journal = {Neural Computing \& Applications},
  year = {2010},
  volume = {19},
  pages = {405--419},
  doi = {10.1007/s00521-009-0307-6},
  issue = {3},
  owner = {memming},
  timestamp = {2009.12.15}
}

@INPROCEEDINGS{Li2010a,
  author = {Lin Li and Il Park and Sohan Seth and Justin C. Sanchez and Jos\'e
	C. Pr\'incipe},
  title = {Neuronal Functional Connectivity Dynamics in Cortex: An {MSC}-based
	Analysis},
  booktitle = {Annual International Conference of the IEEE Engineering in Medicine
	and Biology Society (EMBS)},
  year = {2010},
  owner = {memming},
  timestamp = {2010.06.29}
}

@INPROCEEDINGS{Brockmeier2010a,
  author = {Austin J. Brockmeier and Il Park and Babak Mahmoudi and Justin C.
	Sanchez and Jos\'e C. Pr\'incipe},
  title = {Spatio-Temporal Clustering of Firing Rates for Neural State Estimation},
  booktitle = {Annual International Conference of the IEEE Engineering in Medicine
	and Biology Society (EMBS)},
  year = {2010},
  owner = {memming},
  timestamp = {2010.06.29}
}

@INBOOK{bookch2010a,
  chapter = {9},
  title = {A Reproducing Kernel {H}ilbert Space Framework for Information-Theoretic
	Learning},
  publisher = {Springer},
  year = {2010},
  editor = {Jos\'e C. Pr\'incipe},
  author = {Jos\'e C. Pr\'incipe and Jian Wu Xu and Robert Jenssen and Antonio
	Paiva and Il Park},
  isbn = {978-1441915696},
  owner = {memming},
  timestamp = {2009.12.16}
}

@INPROCEEDINGS{Bobkov2010a,
  author = {Yuriy Bobkov and Kirill Ukhanov and Il Park and Jos\'e C. Pr\'incipe
	and Barry Ache},
  title = {Measuring Ensemble Activity in Lobster {ORN}s through Calcium Imaging},
  booktitle = {Association for Chemoreception (AChemS) Annual Meeting},
  year = {2010},
  abstract = {Lobster ORNs can be imaged in the olfactory organ in situ, thereby
	maintaining the normal polarity of the cells and the ionic environment
	of the olfactory cilia. The preparation gives simultaneous access
	to hundreds of ORNs that are viable for hours, thereby allowing rigorous
	characterization of their steadystate and dynamic properties. Odorants
	change the level of cytoplasmic Ca2+ in a dose-dependent manner in
	ORNs loaded with Ca2+-sensitive indicator either through bath application
	or via a patch electrode. The kinetics and amplitude of the odorantevoked
	Ca2+ signal correlate with the excitatory inward current, the degree
	of membrane depolarization, and the number of evoked action potentials,
	thereby establishing the physiological relevance of the Ca2+ signal.
	Spontaneous periodic Ca2+ transients in many ORNs correlate with
	spontaneous bursts of action potentials measured in single cells
	in the same cluster. We are using signal processing algorithms to
	analyze the level of correlated activity between these ORNs and the
	extent to which periodic calcium oscillations in different ORNs are
	synchronized by common intermittent excitatory input to test the
	predictions of our computational model for ensemble burst coding
	in these cells and the potential relevance of bursting input to olfactory
	scene analysis.},
  owner = {memming},
  timestamp = {2010.07.15}
}

@InProceedings{Seth2009a,
  author    = {Sohan Seth and Il Park and Jos\'e C. Pr\'incipe},
  title     = {A new nonparametric measure of conditional independence},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  year      = {2009},
  doi       = {10.1109/ICASSP.2009.4960250},
  owner     = {memming},
  timestamp = {2009.03.06},
}

@INPROCEEDINGS{Park2009c,
  author = {Il Park and Jos\'e C. Pr\'incipe},
  title = {Significance test for spike trains based on finite point process
	estimation},
  booktitle = {Society for Neuroscience},
  year = {2009},
  owner = {memming},
  timestamp = {2010.01.15}
}

@INPROCEEDINGS{Park2009b,
  author = {Il Park and Yuriy Bobkov and Kirill Ukhanov and Barry W. Ache and
	Jos\'e C. Pr\'incipe},
  title = {Input Driven Synchrony of Oscillating Olfactory Receptor Neurons:
	A Computational Modeling Study},
  booktitle = {Association for Chemoreception (AChemS) Annual Meeting},
  year = {2009},
  owner = {memming},
  timestamp = {2010.01.15}
}

@INPROCEEDINGS{Park2009a,
  author = {Il Park and Murali Rao and Thomas B. DeMarse and Jos\'e C. Pr\'incipe},
  title = {Point Process Model for Precisely Timed Spike Trains.},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational
	and Systems Neuroscience (COSYNE)},
  year = {2009},
  doi = {doi: 10.3389/conf.neuro.06.2009.03.227},
  owner = {memming},
  timestamp = {2009.03.06}
}

@ARTICLE{Paiva2008b,
  author = {Ant\'onio R. C. Paiva and Il Park and Jos\'e C. Pr\'incipe},
  title = {A Reproducing Kernel {H}ilbert Space framework for Spike Trains},
  journal = {Neural Computation},
  year = {2009},
  volume = {21},
  pages = {424--449},
  number = {2},
  month = feb,
  doi = {10.1162/neco.2008.09-07-614},
  owner = {memming},
  timestamp = {2008.10.08}
}

@ARTICLE{Liu2009b,
  author = {Weifeng Liu and Il Park and Jos\'e C. Pr\'incipe},
  title = {An Information Theoretic Approach of Designing Sparse Kernel Adaptive
	Filters},
  journal = {IEEE Transactions on Neural Network},
  year = {2009},
  volume = {20},
  pages = {1950--1961},
  number = {12},
  month = dec,
  doi = {10.1109/TNN.2009.2033676},
  owner = {memming},
  timestamp = {2009.03.22}
}

@ARTICLE{Liu2009a,
  author = {Weifeng Liu and Il Park and Yiwen Wang and Jos\'e C. Pr\'incipe},
  title = {Extended Kernel Recursive Least Squares Algorithm},
  journal = {IEEE Transactions on Signal Processing},
  year = {2009},
  volume = {57},
  pages = {3801--3814},
  number = {10},
  month = oct,
  doi = {10.1109/TSP.2009.2022007},
  owner = {memming},
  timestamp = {2009.03.22}
}

@INPROCEEDINGS{Li2009,
  author = {Lin Li and Sohan Seth and Il Park and Justin C. Sanchez and Jos\'e
	C. Pr\'incipe},
  title = {Estimation and Visualization of Neuronal Functional Connectivity
	in Motor Tasks},
  booktitle = {Annual International Conference of the IEEE Engineering in Medicine
	and Biology Society (EMBS)},
  year = {2009},
  abstract = {In brain-machine interface (BMI) modeling, the firing patterns of
	hundreds of neurons are used to reconstruct a variety of kinematic
	variables. The large number of neurons produces an explosion in the
	number of free parameters, which affects model generalization. This
	paper proposes a model-free measure of pairwise neural dependence
	to rank the importance of neurons in neural to motor mapping. Compared
	to a model-dependent approach such as sensitivity analysis, sixty
	percent of the neurons with the strongest dependence coincide with
	the top 10 most sensitive neurons trained through the model. Using
	this data-driven approach that operates on the input data alone,
	it is possible to perform neuron selection in a more efficient way
	that is not subject to assumptions about decoding models. To further
	understand the functional dependencies that influence neural to motor
	mapping, we use an open source available graph visualization toolkit
	called Prefuse to visualize the neural dependency graph and quantify
	the functional connectivity in motor cortex. This tool when adapted
	to the analysis of neuronal recordings has the potential to easily
	display the relationships in data of large dimension.},
  doi = {10.1109/IEMBS.2009.5333991},
  owner = {memming},
  timestamp = {2009.12.15}
}

@ARTICLE{Dockendorf2008,
  author = {Karl Dockendorf and Il Park and Ping He and Jos\'e C. Pr\'incipe
	and Thomas B. DeMarse},
  title = {Liquid State Machines and Cultured Cortical Networks: The Separation
	Property},
  journal = {Biosystems},
  year = {2009},
  volume = {95},
  pages = {90--97},
  number = {2},
  month = feb,
  doi = {10.1016/j.biosystems.2008.08.001},
  owner = {memming},
  timestamp = {2008.04.15}
}

@INPROCEEDINGS{Bobkov2009a,
  author = {Yuriy Bobkov and Il Park and Kirill Ukhanov and Jos\'e C. Pr\'incipe
	and Barry W. Ache},
  title = {Population coding within an ensemble of rhythmically active primary
	olfactory receptor},
  booktitle = {Society for Neuroscience},
  year = {2009},
  owner = {memming},
  timestamp = {2010.01.15}
}

@ARTICLE{Xu2008,
  author = {Xu, Jian Wu and Paiva, Ant\'onio R. C. and Park, Il and Pr\'incipe,
	Jos\'e C.},
  title = {A Reproducing Kernel {H}ilbert Space Framework for Information-Theoretic
	Learning},
  journal = {IEEE Transactions on Signal Processing},
  year = {2008},
  volume = {56},
  pages = {5891--5902},
  number = {12},
  abstract = {This paper provides a functional analysis perspective of information-theoretic
	learning (ITL) by defining bottom-up a reproducing kernel Hilbert
	space (RKHS) uniquely determined by the symmetric nonnegative definite
	kernel function known as the cross-information potential (CIP). The
	CIP as an integral of the product of two probability density functions
	characterizes similarity between two stochastic functions. We prove
	the existence of a one-to-one congruence mapping between the ITL
	RKHS and the Hilbert space spanned by square integrable probability
	density functions. Therefore, all the statistical descriptors in
	the original information-theoretic learning formulation can be rewritten
	as algebraic computations on deterministic functional vectors in
	the ITL RKHS, instead of limiting the functional view to the estimators
	as is commonly done in kernel methods. A connection between the ITL
	RKHS and kernel approaches interested in quantifying the statistics
	of the projected data is also established.},
  citeulike-article-id = {3744103},
  doi = {10.1109/TSP.2008.2005085},
  keywords = {cnel, itl, rkhs},
  posted-at = {2008-12-03 23:09:57},
  priority = {0}
}

@INPROCEEDINGS{Park2008b,
  author = {Il Park and Jos\'e C. Pr\'incipe},
  title = {Correntropy based Granger causality},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing
	(ICASSP)},
  year = {2008},
  doi = {10.1109/ICASSP.2008.4518432},
  owner = {memming},
  timestamp = {2009.03.06}
}

@ARTICLE{Park2008a,
  author = {Il Park and Ant\'onio R. C. Paiva and Thomas B. DeMarse and Jos\'e
	C. Pr\'incipe},
  title = {An efficient algorithm for continuous-time cross correlogram of spike
	trains},
  journal = {Journal of Neuroscience Methods},
  year = {2008},
  volume = {168},
  pages = {514--523},
  number = {2},
  month = mar,
  abstract = {We propose an efficient algorithm to compute the smoothed correlogram
	for the detection of temporal relationship between two spike trains.
	Unlike the conventional histogram-based correlogram estimations,
	the proposed algorithm operates on continuous time and does not bin
	either the spike train nor the correlogram. Hence it can be more
	precise in detecting the effective delay between two recording sites.
	Moreover, it can take advantage of the higher temporal resolution
	of the spike times provided by the current recording methods. The
	Laplacian kernel for smoothing enables efficient computation of the
	algorithm. We also provide the basic statistics of the estimator
	and a guideline for choosing the kernel size. This new technique
	is demonstrated by estimating the effective delays in a neuronal
	network from synthetic data and recordings of dissociated cortical
	tissue.},
  doi = {10.1016/j.jneumeth.2007.10.005},
  owner = {memming},
  timestamp = {2007.10.21}
}

@inproceedings{Paiva2008c,
  author = {Ant\'onio R. C. Paiva and Il Park and Justin Sanchez and Jos\'e C.
	Pr\'incipe},
  title = {Peri-event Cross-Correlation over Time for Analysis of Interactions
	in Neuronal Firing},
  journal = {International Conference of the IEEE Engineering in Medicine and
	Biology Society (EMBC)},
  year = {2008},
  owner = {memming},
  timestamp = {2008.10.08}
}

@INPROCEEDINGS{Paiva2008a,
  author = {Ant\'onio R. C. Paiva and Il Park and Jos\'e C. Pr\'incipe},
  title = {Reproducing Kernel {H}ilbert Spaces for Spike Train Analysis},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing
	(ICASSP)},
  year = {2008},
  doi = {10.1109/ICASSP.2008.4518834},
  owner = {memming},
  timestamp = {2008.04.15}
}

@MastersThesis{ParkMSThesis2007,
  author    = {Park, Il},
  title     = {Continuous time correlation analysis techniques for spike trains},
  school    = {The University of Florida},
  year      = {2007},
  owner     = {memming},
  timestamp = {2009.04.24},
}

@INPROCEEDINGS{Park2007b,
  author = {Il Park and Ant\'onio R. C. Paiva and Jos\'e C. Pr\'incipe and Thomas
	B. DeMarse},
  title = {An Efficient Computation of Continuous-time Correlogram of Spike
	Trains},
  booktitle = {Frontiers in Systems Neuroscience. Conference Abstract: Computational
	and Systems Neuroscience (COSYNE)},
  year = {2007},
  owner = {memming},
  timestamp = {2010.01.20}
}

@INPROCEEDINGS{Park2007,
  author = {Il Park and Ant\'onio R. C. Paiva and Thomas B. DeMarse and Jose
	C. Pr\'incipe and John Harris},
  title = {A Closed Form Solution for Multiple-Input Spike Based Adaptive Filters},
  booktitle = {IEEE International Joint Conference on Neural Networks (IJCNN)},
  year = {2007},
  owner = {memming},
  timestamp = {2008.04.15}
}

@InProceedings{Paiva2007b,
  author    = {Ant\'onio R. C. Paiva and Il Park and Jos\'e C. Pr\'incipe},
  title     = {Innovating Signal Processing for Spike Train Data},
  booktitle = {International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  year      = {2007},
  address   = {Lyon, France},
  owner     = {memming},
  timestamp = {2009.12.15},
}

@InProceedings{Paiva2007a,
  author    = {Ant\'onio R. C. Paiva and Sudhir Rao and Il Park and Jos\'e C. Pr\'incipe},
  title     = {Spectral Clustering of Synchronous Spike Trains},
  booktitle = {IEEE International Joint Conference on Neural Networks (IJCNN)},
  year      = {2007},
  address   = {Orlando, FL, USA},
  month     = aug,
  owner     = {memming},
  timestamp = {2008.04.15},
}

@INPROCEEDINGS{Park2006,
  author = {Il Park and Dongming Xu and Thomas B. DeMarse and Jos\'e C. Pr\'incipe},
  title = {Modeling of Synchronized Burst in Dissociated Cortical Tissue: An
	Exploration of Parameter Space},
  booktitle = {IEEE International Joint Conference on Neural Networks (IJCNN)},
  year = {2006},
  doi = {10.1109/IJCNN.2006.246734},
  owner = {memming},
  timestamp = {2008.04.15}
}

@InProceedings{Park2005,
  author    = {Il Park and Jong C. Park},
  title     = {Modeling Causality in Biological Pathways for Logical Identification of Drug Targets},
  booktitle = {Bioinfo},
  year      = {2005},
  address   = {Busan, Korea},
  month     = sep,
  owner     = {memming},
  timestamp = {2008.04.15},
}

INPROCEEDINGS{Paiva2008e,
  author = {Ant\'onio R. C. Paiva and Il Park and Jos{\'e} C. Pr{\'i}ncipe},
  title = {Reproducing Kernel {H}ilbert Spaces for Spike Train Analysis},
  booktitle = {Conference on Computational Neuroscience},
  year = {2008},
  owner = {memming},
  timestamp = {2010.01.20}
}

@InBook{Paiva2010a,
  title     = {Optimization in Reproducing Kernel {H}ilbert Spaces of Spike Trains},
  publisher = {Springer},
  year      = {?},
  author    = {Ant\'onio R. C. Paiva and Il Park and Jos\'e C. Pr\'incipe},
  editor    = {W. Art Chaovalitwongse and Panos Pardalos and Petros Xanthopoulos,},
  note      = {(in press)},
  booktitle = {Computational Neuroscience},
  owner     = {memming},
  timestamp = {2010.01.15},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: saveOrderConfig:specified;year;true;bibtexkey;true;author;true;}
